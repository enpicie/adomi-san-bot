name: 'Deploy Infrastructure via HCP Terraform'

on:
  workflow_call:
    inputs:
      app_name:
        description: Name of the application
        required: true
        type: string
      sqs_worker_name:
        description: Name of the application
        required: true
        type: string
      deployment_env:
        description: Environment to deploy to (e.g., dev, prod)
        required: true
        type: string
      python_runtime:
        description: Python runtime for Lambda (e.g., 3.11)
        required: true
        type: string
      aws_region:
        description: AWS Region to deploy to
        required: true
        type: string
      artifact_s3_bucket:
        description: S3 bucket name for storing build artifacts
        required: true
        type: string
      hcp_terraform_org:
        description: HCP Terraform Organization name
        required: true
        type: string
      hcp_terraform_workspace:
        description: HCP Terraform Workspace name
        required: true
        type: string
      hcp_terraform_project:
        description: HCP Terraform Project name
        required: true
        type: string
      app_version:
        description: Version of the application
        required: false
        type: string
      discord_public_key:
        description: Public key for Discord verification (env-specific and used by Terraform)
        required: true
        type: string
    secrets:
      AWS_S3_ARTIFACT_UPLOAD_ROLE_ARN:
        required: true
      TF_API_TOKEN:
        required: true
      STARTGG_API_TOKEN:
        required: true
      DISCORD_BOT_TOKEN:
        required: true

jobs:
  deploy-via-hcp-terraform:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      ## ðŸ” Check for Code/Infra Changes
      # Calculate the hash for `src` and `terraform` directories to decide if a deployment is needed.
      - name: Determine if Deployment is Required ðŸš€
        id: change_check
        run: |
          # The key for caching is a hash of the content of both directories
          CACHE_KEY_SRC="src-v1-${{ hashFiles('src/**') }}"
          CACHE_KEY_TF="tf-v1-${{ hashFiles('terraform/**') }}"

          # A combined key to check for *any* change in a single step (optional, but helpful for logs)
          DEPLOY_KEY="deploy-v1-${{ hashFiles('src/**') }}-${{ hashFiles('terraform/**') }}"

          # Check if the combined key exists in the cache
          if ${{ github.event.pull_request.head.sha == '' }}; then
            echo "::notice title=Deployment Check::Running on a branch or tag, proceeding with change detection."
            echo "cache-key=$DEPLOY_KEY" >> $GITHUB_OUTPUT
          else
            echo "::notice title=Deployment Check::Running on a pull request. Deployment will run to ensure validation."
            echo "cache-key=force-run-on-pr" >> $GITHUB_OUTPUT
          fi

      - name: Check Cache and Set Deployment Flag
        id: check_cache
        uses: actions/cache/restore@v3
        with:
          path: /tmp/deploy_status # Dummy path as we only care about the cache hit
          key: ${{ steps.change_check.outputs.cache-key }}

      - name: Finalize Deployment Decision
        id: deployment_decision
        run: |
          if [[ "${{ steps.check_cache.outputs.cache-hit }}" == "true" ]]; then
            echo "Deployment Skipped: No changes detected in src or terraform directories."
            echo "deploy_required=false" >> $GITHUB_OUTPUT
          elif [[ "${{ steps.change_check.outputs.cache-key }}" == "force-run-on-pr" ]]; then
            echo "Deployment will run: Workflow triggered by a Pull Request."
            echo "deploy_required=true" >> $GITHUB_OUTPUT
          else
            echo "Deployment Required: Changes detected in src or terraform directories."
            echo "deploy_required=true" >> $GITHUB_OUTPUT
          fi

      # The rest of the workflow steps are conditionally executed based on the flag.

      - name: Setup HCP Terraform Workspace
        if: steps.deployment_decision.outputs.deploy_required == 'true'
        id: setup_workspace
        uses: enpicie/action-workflow-hcp-terraform-workspace-setup@v1.1.0
        with:
          tfc_organization: ${{ inputs.hcp_terraform_org }}
          tfc_workspace: ${{ inputs.hcp_terraform_workspace }}
          tfc_project: ${{ inputs.hcp_terraform_project }}
          tfc_token: ${{ secrets.TF_API_TOKEN }}

      - name: Attach IAM Permissions Variable Set to this Workspace
        if: steps.deployment_decision.outputs.deploy_required == 'true'
        uses: enpicie/action-workflow-hcp-terraform-var-set-attach@v1.0.1
        with:
          tfc_organization: ${{ inputs.hcp_terraform_org }}
          tfc_workspace_id: ${{ steps.setup_workspace.outputs.workspace_id }}
          tfc_token: ${{ secrets.TF_API_TOKEN }}
          var_set_name: ${{ vars.AWS_TF_ROLE_VARSET_LAMB_APIGW_DDB_SQS }}

      # The following Build/Upload steps only need to run if code in 'src' or 'terraform' changed.
      # If only 'src' changed, it's needed for the new zip. If only 'terraform' changed, the build steps run to generate the layer S3 keys again for Terraform,
      # but the layer zip might already exist. Running them conditionally is safe and cleaner.

      - name: Build and Upload Lambda Layer Artifact to S3 - Main Lambda
        if: steps.deployment_decision.outputs.deploy_required == 'true'
        id: build_layer
        uses: enpicie/action-workflow-build-python-lambda-layer-zip@v2.0.4
        with:
          requirements_path: ./requirements.txt # Path relative the repository root
          app_name: ${{ inputs.app_name }}
          python_runtime: ${{ inputs.python_runtime }}
          s3_layer_bucket_name: ${{ inputs.artifact_s3_bucket }} # Store layer in same bucket for env as artifacts
          aws_role_arn: ${{ secrets.AWS_S3_ARTIFACT_UPLOAD_ROLE_ARN }}

      - name: Build and Upload Lambda Layer Artifact to S3 - SQS Worker Lambda
        if: steps.deployment_decision.outputs.deploy_required == 'true'
        id: build_layer_worker
        uses: enpicie/action-workflow-build-python-lambda-layer-zip@v2.0.4
        with:
          requirements_path: ./jobs/remove_role/requirements.txt # Path relative the repository root
          app_name: ${{ inputs.sqs_worker_name }}
          python_runtime: ${{ inputs.python_runtime }}
          s3_layer_bucket_name: ${{ inputs.artifact_s3_bucket }}
          aws_role_arn: ${{ secrets.AWS_S3_ARTIFACT_UPLOAD_ROLE_ARN }}

      - name: Upload Latest Lambda Zip to S3 - Main Lambda
        if: steps.deployment_decision.outputs.deploy_required == 'true'
        uses: enpicie/action-workflow-upload-lambda-zip@v0.3.0
        with:
          source_directory: ./src
          app_name: ${{ inputs.app_name }}
          artifact_name: ${{ inputs.app_name }}-latest
          s3_bucket_name: ${{ inputs.artifact_s3_bucket }}
          aws_role_arn: ${{ secrets.AWS_S3_ARTIFACT_UPLOAD_ROLE_ARN }}

      - name: Upload Version Lambda Zip to S3 - Main Lambda
        if: ${{ inputs.app_version != '' && steps.deployment_decision.outputs.deploy_required == 'true' }}
        uses: enpicie/action-workflow-upload-lambda-zip@v0.3.0
        with:
          source_directory: ./src
          app_name: ${{ inputs.app_name }}
          artifact_name: ${{ inputs.app_name }}-${{ inputs.app_version }}
          s3_bucket_name: ${{ inputs.artifact_s3_bucket }}
          aws_role_arn: ${{ secrets.AWS_S3_ARTIFACT_UPLOAD_ROLE_ARN }}

      - name: Upload Latest Lambda Zip to S3 - SWS Worker Lambda
        if: steps.deployment_decision.outputs.deploy_required == 'true'
        uses: enpicie/action-workflow-upload-lambda-zip@v0.3.0
        with:
          source_directory: ./jobs/remove_role
          app_name: ${{ inputs.sqs_worker_name }}
          artifact_name: ${{ inputs.sqs_worker_name }}-latest
          s3_bucket_name: ${{ inputs.artifact_s3_bucket }}
          aws_role_arn: ${{ secrets.AWS_S3_ARTIFACT_UPLOAD_ROLE_ARN }}

      - name: Log Terraform input variables
        if: steps.deployment_decision.outputs.deploy_required == 'true'
        shell: bash
        run: |
          echo "Logging Terraform variables:"
          echo "TF_VAR_app_name=${TF_VAR_app_name}"
          echo "TF_VAR_aws_region=${TF_VAR_aws_region}"
          echo "TF_VAR_python_runtime=${TF_VAR_python_runtime}"
          echo "TF_VAR_deployment_env=${TF_VAR_deployment_env}"
          echo "TF_VAR_bucket_name=${TF_VAR_bucket_name}"
          echo "TF_VAR_app_lambda_layer_s3_key=${TF_VAR_app_lambda_layer_s3_key}"
          echo "TF_VAR_app_lambda_layer_hash_s3_key=${TF_VAR_app_lambda_layer_hash_s3_key}"
          echo "TF_VAR_worker_lambda_layer_s3_key=${TF_VAR_worker_lambda_layer_s3_key}"
          echo "TF_VAR_worker_lambda_layer_hash_s3_key=${TF_VAR_worker_lambda_layer_hash_s3_key}"
          echo "TF_VAR_discord_public_key=${TF_VAR_discord_public_key}"
        env:
          TF_VAR_app_name: '${{ inputs.app_name }}'
          TF_VAR_aws_region: '${{ inputs.aws_region }}'
          TF_VAR_python_runtime: '${{ inputs.python_runtime }}'
          TF_VAR_deployment_env: '${{ inputs.deployment_env }}'
          TF_VAR_bucket_name: '${{ inputs.artifact_s3_bucket }}'
          # Note: The 'build_layer' and 'build_layer_worker' steps must run before this for these outputs to exist.
          TF_VAR_app_lambda_layer_s3_key: '${{ steps.build_layer.outputs.s3_artifact_key }}'
          TF_VAR_app_lambda_layer_hash_s3_key: '${{ steps.build_layer.outputs.s3_artifact_hash_key }}'
          TF_VAR_worker_lambda_layer_s3_key: '${{ steps.build_layer_worker.outputs.s3_artifact_key }}'
          TF_VAR_worker_lambda_layer_hash_s3_key: '${{ steps.build_layer_worker.outputs.s3_artifact_hash_key }}'
          TF_VAR_discord_public_key: '${{ inputs.discord_public_key }}'

      - name: Deploy Application Config via HCP Terraform
        if: steps.deployment_decision.outputs.deploy_required == 'true'
        uses: enpicie/action-workflow-hcp-terraform-run@v1.1.1
        with:
          terraform_directory: ./terraform
          tfc_organization: ${{ inputs.hcp_terraform_org }}
          tfc_workspace: ${{ inputs.hcp_terraform_workspace }}
          tfc_token: ${{ secrets.TF_API_TOKEN }}
        env:
          # Need explicit quotes to pass strings as string literaly correctly
          TF_VAR_app_name: '"${{ inputs.app_name }}"'
          TF_VAR_sqs_worker_name: '"${{ inputs.sqs_worker_name }}"'
          TF_VAR_aws_region: '"${{ inputs.aws_region }}"'
          TF_VAR_python_runtime: '"${{ inputs.python_runtime }}"'
          TF_VAR_deployment_env: '"${{ inputs.deployment_env }}"'
          TF_VAR_bucket_name: '"${{ inputs.artifact_s3_bucket }}"'
          TF_VAR_app_lambda_layer_s3_key: '"${{ steps.build_layer.outputs.s3_artifact_key }}"'
          TF_VAR_app_lambda_layer_hash_s3_key: '"${{ steps.build_layer.outputs.s3_artifact_hash_key }}"'
          TF_VAR_worker_lambda_layer_s3_key: '"${{ steps.build_layer_worker.outputs.s3_artifact_key }}"'
          TF_VAR_worker_lambda_layer_hash_s3_key: '"${{ steps.build_layer_worker.outputs.s3_artifact_hash_key }}"'
          TF_VAR_discord_public_key: '"${{ inputs.discord_public_key }}"'
          TF_VAR_startgg_api_token: '"${{ secrets.STARTGG_API_TOKEN }}"'
          TF_VAR_discord_bot_token: '"${{ secrets.DISCORD_BOT_TOKEN }}"'

      ## ðŸ’¾ Cache Success for Future Skips
      # Save the cache only if the deployment was successful (i.e., `deploy_required` was true and the steps passed)
      - name: Save Deployment Cache Key
        uses: actions/cache/save@v3
        if: ${{ always() && steps.deployment_decision.outputs.deploy_required == 'true' && success() }}
        with:
          path: /tmp/deploy_status
          key: ${{ steps.change_check.outputs.cache-key }}
